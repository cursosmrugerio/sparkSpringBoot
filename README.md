# E-commerce Analytics con Apache Spark y Spring Boot

Sistema de anÃ¡lisis de datos de e-commerce utilizando Apache Spark para procesamiento distribuido integrado con Spring Boot.

## ğŸ“‹ Bloque 1: Fundamentos y ConfiguraciÃ³n - COMPLETADO âœ…

### Stack TecnolÃ³gico
- **Java:** 17 LTS
- **Apache Spark:** 3.5.0
- **Spring Boot:** 3.2.0
- **PostgreSQL:** 15
- **Docker:** Cluster Spark + PostgreSQL

---

## ğŸš€ Quick Start (3 Pasos)

### OpciÃ³n 1: Script Automatizado (Recomendado)

```bash
# 1. Ejecutar script de inicio
./start.sh
```

El script automÃ¡ticamente:
- âœ… Verifica Java 17
- âœ… Compila el proyecto
- âœ… Inicia PostgreSQL (Docker)
- âœ… Ejecuta la aplicaciÃ³n con la configuraciÃ³n correcta

### OpciÃ³n 2: EjecuciÃ³n Manual

#### Paso 1: Instalar Java 17

âš ï¸ **CRÃTICO**: Java 17 es obligatorio (Java 18+ NO funciona con Spark 3.5.0)

```bash
# Instalar con SDKMAN (recomendado)
curl -s "https://get.sdkman.io" | bash
source "$HOME/.sdkman/bin/sdkman-init.sh"
sdk install java 17.0.13-tem
sdk use java 17.0.13-tem
java -version  # Debe mostrar: openjdk version "17.0.13"
```

#### Paso 2: Iniciar PostgreSQL

```bash
# Levantar PostgreSQL con Docker
docker compose up -d postgres

# Verificar que estÃ© corriendo
docker ps | grep postgres
```

**Credenciales PostgreSQL:**
- Host: `localhost:5432`
- Usuario: `sparkuser`
- Password: `sparkpass`
- Base de datos: `ecommerce_analytics`

#### Paso 3: Compilar y Ejecutar

**OpciÃ³n A: Con Maven (Desarrollo)**
```bash
# Configurar Java 17
export JAVA_HOME=~/.sdkman/candidates/java/17.0.13-tem
export PATH=$JAVA_HOME/bin:$PATH

# Compilar y ejecutar
mvn clean spring-boot:run -Dspring-boot.run.profiles=local
```

**OpciÃ³n B: Con JAR (ProducciÃ³n)**
```bash
# 1. Compilar
mvn clean package -DskipTests

# 2. Ejecutar con JVM arguments (OBLIGATORIO para Java 17)
export JAVA_HOME=~/.sdkman/candidates/java/17.0.13-tem
export PATH=$JAVA_HOME/bin:$PATH

java --add-opens java.base/java.lang=ALL-UNNAMED \
     --add-opens java.base/sun.nio.ch=ALL-UNNAMED \
     --add-opens java.base/sun.util.calendar=ALL-UNNAMED \
     -jar target/analytics-1.0.0.jar \
     --spring.profiles.active=local
```

âš ï¸ **IMPORTANTE**: Los argumentos `--add-opens` son obligatorios cuando ejecutas el JAR directamente. Sin ellos obtendrÃ¡s `IllegalAccessError`.

**AplicaciÃ³n disponible en:** http://localhost:8080

---

## ğŸ“– DocumentaciÃ³n Completa

- **GuÃ­a completa Bloque 1**: `docs/BLOQUE1_GUIA_COMPLETA.md`
- **Troubleshooting detallado**: `docs/BLOQUE1_GUIA_COMPLETA.md#consideraciones-tÃ©cnicas-crÃ­ticas`

---

## ğŸ§ª Verificar InstalaciÃ³n

### Endpoints Disponibles

#### 1. Health Check
```bash
curl http://localhost:8080/api/data/health
```

Respuesta esperada:
```json
{
  "status": "OK",
  "spark": "Running",
  "message": "Spark integration is working correctly"
}
```

#### 2. Leer Transacciones
```bash
curl http://localhost:8080/api/data/transactions?limit=5
```

#### 3. Leer Productos
```bash
curl http://localhost:8080/api/data/products?limit=5
```

#### 4. Leer Clientes
```bash
curl http://localhost:8080/api/data/customers?limit=5
```

---

## ğŸ“Š Datasets de Ejemplo

UbicaciÃ³n: `./data/`

- **transactions.csv**: 20 transacciones de ejemplo
- **products.csv**: 17 productos con categorÃ­as
- **customers.csv**: 17 clientes con informaciÃ³n demogrÃ¡fica

---

## ğŸ—ï¸ Estructura del Proyecto

```
ecommerce-analytics/
â”œâ”€â”€ docker-compose.yml              # Infraestructura Spark + PostgreSQL
â”œâ”€â”€ pom.xml                         # Dependencias Maven
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ transactions.csv            # Dataset transacciones
â”‚   â”œâ”€â”€ products.csv                # Dataset productos
â”‚   â””â”€â”€ customers.csv               # Dataset clientes
â”œâ”€â”€ src/main/java/com/ecommerce/analytics/
â”‚   â”œâ”€â”€ EcommerceAnalyticsApplication.java    # Main class
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ SparkConfig.java                  # ConfiguraciÃ³n SparkSession
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â””â”€â”€ DataReaderService.java            # Servicio lectura de datos
â”‚   â””â”€â”€ controller/
â”‚       â””â”€â”€ DataExplorationController.java    # REST endpoints
â””â”€â”€ src/main/resources/
    â”œâ”€â”€ application.yml               # ConfiguraciÃ³n base
    â”œâ”€â”€ application-local.yml         # Perfil local
    â””â”€â”€ application-docker.yml        # Perfil Docker
```

---

## ğŸ”‘ Conceptos Clave Implementados

### 1. SparkSession como Bean de Spring
- ConfiguraciÃ³n en `SparkConfig.java`
- Perfiles separados: `local` y `docker`
- InyecciÃ³n de dependencias con `@Autowired`

### 2. Lectura de Datos CSV
- Headers automÃ¡ticos con `option("header", "true")`
- Inferencia de schema con `option("inferSchema", "true")`
- DataFrames tipados como `Dataset<Row>`

### 3. Operaciones BÃ¡sicas de Spark
- `count()`: Contar registros (acciÃ³n)
- `show()`: Mostrar datos (acciÃ³n)
- `printSchema()`: Ver estructura de datos
- **Lazy Evaluation**: Las transformaciones no se ejecutan hasta una acciÃ³n

### 4. IntegraciÃ³n Spring Boot + Spark
- Servicio `DataReaderService` con inyecciÃ³n de SparkSession
- Endpoints REST que exponen resultados de Spark
- ConversiÃ³n de DataFrames a JSON para APIs

---

## ğŸ¯ Ejercicios PrÃ¡cticos del Bloque 1

### Ejercicio 1: Explorar Productos
```bash
# Ver primeras 20 filas de productos
curl http://localhost:8080/api/data/products?limit=20

# Observar el schema inferido
# Contar total de productos
```

### Ejercicio 2: Entender Lazy Evaluation
Agregar logging en `DataReaderService.java` para ver cuÃ¡ndo se ejecutan las operaciones:
```java
System.out.println("Leyendo CSV..."); // Se ejecuta inmediatamente
Dataset<Row> df = sparkSession.read()...
System.out.println("DataFrame creado"); // AÃºn no se leyÃ³ el archivo

long count = df.count(); // AQUÃ se ejecuta la lectura
System.out.println("Archivo leÃ­do y contado: " + count);
```

---

## ğŸ“ ConfiguraciÃ³n de Perfiles

### Perfil Local (`application-local.yml`)
- Spark Master: `local[*]` (todos los cores locales)
- PostgreSQL: `localhost:5432`
- Path de datos: `./data`

### Perfil Docker (`application-docker.yml`)
- Spark Master: `spark://spark-master:7077`
- PostgreSQL: `postgres:5432`
- Path de datos: `/data`

---

## ğŸ³ Comandos Ãštiles de Docker

```bash
# Ver logs del cluster Spark
docker logs spark-master
docker logs spark-worker-1

# Detener servicios
docker-compose down

# Reiniciar servicios
docker-compose restart

# Eliminar todo (incluyendo volÃºmenes)
docker-compose down -v
```

---

## âœ… Checklist de VerificaciÃ³n Bloque 1

- [x] Proyecto Spring Boot creado con Maven
- [x] Dependencias de Spark agregadas
- [x] Docker Compose configurado (Spark + PostgreSQL)
- [x] SparkSession configurado como Bean
- [x] Perfiles `local` y `docker` funcionando
- [x] Datasets CSV creados
- [x] Servicio de lectura de datos implementado
- [x] Endpoints REST expuestos
- [x] Health check funcionando
- [x] Primera lectura exitosa de datos

---

## ğŸ“ Conceptos Aprendidos

1. **Arquitectura Spark**: Driver, Executors, Cluster Manager
2. **RDD vs DataFrame vs Dataset**: Diferencias y cuÃ¡ndo usar cada uno
3. **Lazy Evaluation**: Transformaciones vs Acciones
4. **SparkSession**: Punto de entrada para trabajar con Spark
5. **IntegraciÃ³n Spring Boot**: InyecciÃ³n de dependencias y configuraciÃ³n por perfiles

---

## ğŸ”œ PrÃ³ximos Pasos: Bloque 2

En el siguiente bloque implementaremos:
- Transformaciones avanzadas (select, filter, groupBy)
- Agregaciones de negocio (ventas por categorÃ­a, top productos)
- Joins entre datasets
- Spark SQL
- MÃ¡s endpoints REST con filtros dinÃ¡micos

---

## ğŸ†˜ Troubleshooting

Para problemas de compatibilidad y errores comunes, consulta la **guÃ­a completa de troubleshooting** en:
ğŸ“– `docs/CONFIGURACION_CORRECTA.md`

### Errores Comunes:

#### âŒ Java 18+ Incompatibilidad
```
Error: getSubject is supported only if a security manager is allowed
```
**SoluciÃ³n**: Usar Java 17. Ver instrucciones de instalaciÃ³n arriba.

#### âŒ ANTLR Version Conflict
```
Error: Could not deserialize ATN with version 3 (expected 4)
```
**SoluciÃ³n**: El pom.xml incluye `<dependencyManagement>` que fuerza ANTLR 4.9.3. Si persiste, ejecutar `mvn clean package`.

#### âŒ Puerto 8080 en uso
```bash
# Matar proceso en puerto 8080
lsof -ti:8080 | xargs kill -9

# O cambiar puerto en application.yml
server:
  port: 8081
```

#### âŒ PostgreSQL no conecta
```bash
# Verificar contenedor
docker ps | grep postgres

# Reiniciar PostgreSQL
docker-compose restart postgres

# Ver logs
docker logs postgres-db
```

#### âŒ Module Access Errors (Java 17)
**Error:**
```
IllegalAccessError: class org.apache.spark.storage.StorageUtils$ cannot access
class sun.nio.ch.DirectBuffer
```

**SoluciÃ³n:**
- Si ejecutas con Maven: Los JVM arguments se aplican automÃ¡ticamente desde `pom.xml`
- Si ejecutas el JAR: Debes agregar los argumentos `--add-opens` manualmente (ver comando arriba)
- SoluciÃ³n rÃ¡pida: Usa el script `./start.sh`

---

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n del Proyecto
- ğŸ“– **PRD Completo**: `PRD.md` - Product Requirements Document
- ğŸ“˜ **GuÃ­a Bloque 1**: `docs/BLOQUE1_GUIA_COMPLETA.md` - Tutorial completo con ejemplos
- ğŸ”§ **Troubleshooting**: Ver secciÃ³n "Consideraciones TÃ©cnicas CrÃ­ticas" en guÃ­a Bloque 1

### DocumentaciÃ³n Oficial
- [Apache Spark 3.5.0 Documentation](https://spark.apache.org/docs/3.5.0/)
- [Spring Boot 3.2.0 Reference](https://docs.spring.io/spring-boot/docs/3.2.0/reference/html/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)

---

**VersiÃ³n:** 1.1 - Bloque 1 Completado + Mejoras de DocumentaciÃ³n
**Fecha:** Octubre 2025
**Ãšltima ActualizaciÃ³n:** Octubre 2025 - Agregado Quick Start y script automatizado

**Cambios en v1.1:**
- âœ… Agregada secciÃ³n Quick Start con script automatizado (`start.sh`)
- âœ… Comandos de ejecuciÃ³n validados (Maven y JAR)
- âœ… DocumentaciÃ³n de JVM arguments obligatorios para Java 17
- âœ… Enlaces a documentaciÃ³n completa y troubleshooting
