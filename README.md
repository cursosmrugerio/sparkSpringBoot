# E-commerce Analytics con Apache Spark y Spring Boot

Sistema de an√°lisis de datos de e-commerce utilizando Apache Spark para procesamiento distribuido integrado con Spring Boot.

## üìã Progreso de Capacitaci√≥n

### ‚úÖ Bloque 1: Fundamentos y Configuraci√≥n - COMPLETADO
- Setup de infraestructura (Java 17, Docker, PostgreSQL)
- Configuraci√≥n Spark + Spring Boot
- Primera lectura de datos (CSVs)
- API REST b√°sica (4 endpoints)

### ‚úÖ Bloque 2: Transformaciones y An√°lisis - COMPLETADO
- Transformaciones avanzadas (select, filter, groupBy, orderBy)
- Agregaciones de negocio (sum, avg, count, max, min)
- Joins entre datasets (transactions + products + customers)
- Window Functions (rankings)
- API REST completa (7 endpoints nuevos)

### ‚úÖ Bloque 3: Procesamiento Avanzado y Optimizaci√≥n - COMPLETADO
- User Defined Functions (UDFs) personalizadas (5 UDFs)
- Optimizaci√≥n con cach√© y broadcast joins
- Detecci√≥n de fraude con an√°lisis estad√≠stico
- Persistencia bidireccional PostgreSQL
- API REST extendida (20 endpoints nuevos)

### ‚úÖ Bloque 4: Batch Processing y Automatizaci√≥n - COMPLETADO
- ETL Pipeline completo (Extract-Transform-Load)
- Procesamiento incremental automatizado
- Jobs programados con Spring @Scheduled
- Generaci√≥n autom√°tica de reportes diarios
- Sistema de tracking de ejecuciones de jobs
- API REST para gesti√≥n de batch jobs (8 endpoints)
- Dual storage: PostgreSQL + Parquet files

### Stack Tecnol√≥gico
- **Java:** 17 LTS (OBLIGATORIO - Java 18+ incompatible)
- **Apache Spark:** 3.5.0
- **Spring Boot:** 2.7.18 (downgraded from 3.2.0 for ANTLR compatibility)
- **PostgreSQL:** 15
- **ANTLR:** 4.9.3 (forced version for Spark 3.5.0 compatibility)
- **Docker:** PostgreSQL container

---

## üöÄ Quick Start (3 Pasos)

### Opci√≥n 1: Script Automatizado (Recomendado)

```bash
# 1. Ejecutar script de inicio
./start.sh
```

El script autom√°ticamente:
- ‚úÖ Verifica Java 17
- ‚úÖ Compila el proyecto
- ‚úÖ Inicia PostgreSQL (Docker)
- ‚úÖ Ejecuta la aplicaci√≥n con la configuraci√≥n correcta

### Opci√≥n 2: Ejecuci√≥n Manual

#### Paso 1: Instalar Java 17

‚ö†Ô∏è **CR√çTICO**: Java 17 es obligatorio (Java 18+ NO funciona con Spark 3.5.0)

```bash
# Instalar con SDKMAN (recomendado)
curl -s "https://get.sdkman.io" | bash
source "$HOME/.sdkman/bin/sdkman-init.sh"
sdk install java 17.0.13-tem
sdk use java 17.0.13-tem
java -version  # Debe mostrar: openjdk version "17.0.13"
```

#### Paso 2: Iniciar PostgreSQL

```bash
# Levantar PostgreSQL con Docker
docker compose up -d postgres

# Verificar que est√© corriendo
docker ps | grep postgres
```

**Credenciales PostgreSQL:**
- Host: `localhost:5432`
- Usuario: `sparkuser`
- Password: `sparkpass`
- Base de datos: `ecommerce_analytics`

#### Paso 3: Compilar y Ejecutar

**Opci√≥n A: Con Maven (Desarrollo)**
```bash
# Configurar Java 17
export JAVA_HOME=~/.sdkman/candidates/java/17.0.13-tem
export PATH=$JAVA_HOME/bin:$PATH

# Compilar y ejecutar
mvn clean spring-boot:run -Dspring-boot.run.profiles=local
```

**Opci√≥n B: Con JAR (Producci√≥n)**
```bash
# 1. Compilar
mvn clean package -DskipTests

# 2. Ejecutar con JVM arguments (OBLIGATORIO para Java 17)
export JAVA_HOME=~/.sdkman/candidates/java/17.0.13-tem
export PATH=$JAVA_HOME/bin:$PATH

java --add-opens java.base/java.lang=ALL-UNNAMED \
     --add-opens java.base/sun.nio.ch=ALL-UNNAMED \
     --add-opens java.base/sun.util.calendar=ALL-UNNAMED \
     -jar target/analytics-1.0.0.jar \
     --spring.profiles.active=local
```

‚ö†Ô∏è **IMPORTANTE**: Los argumentos `--add-opens` son obligatorios cuando ejecutas el JAR directamente. Sin ellos obtendr√°s `IllegalAccessError`.

**Aplicaci√≥n disponible en:** http://localhost:8080

---

## üìñ Documentaci√≥n Completa

- **Gu√≠a Bloque 1**: `docs/BLOQUE1_GUIA_COMPLETA.md` - Fundamentos y configuraci√≥n
- **Gu√≠a Bloque 2**: `docs/BLOQUE2_GUIA_COMPLETA.md` - Transformaciones y an√°lisis ‚ú® NUEVO
- **Troubleshooting**: Ver secci√≥n "Consideraciones T√©cnicas Cr√≠ticas" en Bloque 1

---

## üß™ API REST - Endpoints Disponibles

### üìå Bloque 1: Exploraci√≥n B√°sica de Datos

#### Health Check
```bash
curl http://localhost:8080/api/data/health
```

#### Listar Datos (Transacciones, Productos, Clientes)
```bash
curl http://localhost:8080/api/data/transactions?limit=5
curl http://localhost:8080/api/data/products?limit=5
curl http://localhost:8080/api/data/customers?limit=5
```

---

### üìä Bloque 2: An√°lisis de Negocio

#### 1. Ventas por Categor√≠a
```bash
curl http://localhost:8080/api/sales/by-category
```

**Respuesta:**
```json
[
    {
        "category": "Electronics",
        "totalSales": 2314.83,
        "totalQuantity": 17,
        "avgAmount": 210.44,
        "transactionCount": 11
    }
]
```

**Operaciones Spark:** JOIN (transactions + products), groupBy, agg(sum, avg, count)

#### 2. Top Productos M√°s Vendidos
```bash
curl "http://localhost:8080/api/products/top-selling?limit=10"
```

**Respuesta:**
```json
[
    {
        "productId": "PROD011",
        "productName": "Gaming Chair",
        "category": "Furniture",
        "totalSales": 799.99,
        "quantity": 1,
        "rank": 1
    }
]
```

**Operaciones Spark:** Window Functions (row_number), JOIN, groupBy, orderBy

#### 3. Estad√≠sticas Generales
```bash
curl http://localhost:8080/api/sales/statistics
```

**Respuesta:**
```json
{
    "totalRevenue": 4049.61,
    "avgTicket": 202.48,
    "maxTransaction": 799.99,
    "minTransaction": 29.98,
    "totalTransactions": 20,
    "uniqueCustomers": 17
}
```

**Operaciones Spark:** M√∫ltiples agregaciones (sum, avg, max, min, count, countDistinct)

#### 4. Ventas por Regi√≥n
```bash
# Sin filtros
curl http://localhost:8080/api/sales/by-region

# Con filtros de fecha
curl "http://localhost:8080/api/sales/by-region?startDate=2024-10-01&endDate=2024-10-31"
```

**Operaciones Spark:** Filtrado din√°mico, groupBy, countDistinct

#### 5. Resumen Diario de Ventas
```bash
curl "http://localhost:8080/api/sales/daily-summary?startDate=2024-10-01&endDate=2024-10-02"
```

**Respuesta:**
```json
[
    {
        "date": "2024-10-01",
        "totalSales": 1464.80,
        "transactionCount": 10,
        "avgTicket": 146.48,
        "uniqueCustomers": 9
    }
]
```

**Operaciones Spark:** Funciones de fecha (to_date), filtrado por rango, groupBy

#### 6. Productos por Categor√≠a con Ventas
```bash
curl http://localhost:8080/api/products/by-category/Electronics
```

**Operaciones Spark:** Filter, JOIN, groupBy, orderBy

#### 7. Analytics de Producto Espec√≠fico
```bash
curl http://localhost:8080/api/products/PROD001/analytics
```

**Respuesta:**
```json
{
    "productId": "PROD001",
    "productName": "Wireless Mouse",
    "category": "Electronics",
    "price": 29.99,
    "stock": 150,
    "totalRevenue": 119.96,
    "totalQuantity": 4,
    "transactionCount": 2,
    "avgTicket": 59.98,
    "uniqueCustomers": 2
}
```

**Operaciones Spark:** Filter, JOIN, agregaciones espec√≠ficas

---

### üî¨ Bloque 3: Optimizaci√≥n y Detecci√≥n de Fraude ‚ú® NUEVO

#### 1. Cach√© de Datasets
```bash
# Cachear datasets en memoria para mejor performance
curl -X POST http://localhost:8080/api/optimization/cache

# Ver informaci√≥n de cach√©
curl http://localhost:8080/api/optimization/cache/info

# Liberar cach√©
curl -X DELETE http://localhost:8080/api/optimization/cache
```

**Operaciones Spark:** `persist(StorageLevel.MEMORY_AND_DISK)`, cache management

#### 2. Transacciones Enriquecidas con UDFs
```bash
curl "http://localhost:8080/api/optimization/transactions/enriched?limit=3"
```

**Respuesta:**
```json
[
    {
        "transactionId": "TXN001",
        "amount": 59.98,
        "quantity": 2,
        "fraudRisk": "BAJO_RIESGO",
        "amountCategory": "MEDIO",
        "discountPct": 5.0,
        "amountWithDiscount": 56.98
    }
]
```

**UDFs Aplicadas:**
- `detect_fraud`: Categoriza riesgo (BAJO/MEDIO/ALTO_RIESGO)
- `categorize_amount`: Clasifica monto (BAJO/MEDIO/ALTO/MUY_ALTO)
- `calculate_discount`: Calcula descuento basado en monto

#### 3. Detecci√≥n de Fraude
```bash
# Detectar transacciones sospechosas
curl "http://localhost:8080/api/fraud/detect?stdDevThreshold=2.0&limit=10"

# Detectar y guardar en PostgreSQL
curl -X POST "http://localhost:8080/api/fraud/detect-and-save?stdDevThreshold=2.5"

# Ver estad√≠sticas de fraude
curl http://localhost:8080/api/fraud/statistics
```

**Respuesta (detect):**
```json
[
    {
        "transactionId": "TXN013",
        "amount": 799.99,
        "quantity": 1,
        "deviation": 2.93,
        "isOutlier": true,
        "fraudRisk": "BAJO_RIESGO",
        "amountCategory": "MUY_ALTO"
    }
]
```

**Algoritmo:** Detecci√≥n de outliers usando desviaci√≥n est√°ndar (Z-score)

#### 4. An√°lisis de Patrones de Fraude
```bash
# Patrones por cliente
curl "http://localhost:8080/api/fraud/customer-patterns?stdDevThreshold=2.0&limit=5"

# Patrones por producto
curl "http://localhost:8080/api/fraud/product-patterns?stdDevThreshold=2.0&limit=5"

# Detectar duplicados sospechosos
curl "http://localhost:8080/api/fraud/duplicates?limit=10"
```

#### 5. Gesti√≥n de Alertas de Fraude
```bash
# Obtener todas las alertas
curl http://localhost:8080/api/fraud/alerts

# Obtener alertas de alto riesgo
curl http://localhost:8080/api/fraud/alerts/high-risk

# Marcar alerta como revisada
curl -X PUT http://localhost:8080/api/fraud/alerts/1/review
```

#### 6. Broadcast Joins Optimizados
```bash
curl "http://localhost:8080/api/optimization/transactions/broadcast-join?limit=5"
```

**Operaci√≥n:** Join optimizado usando `broadcast()` para tablas peque√±as (productos)

#### 7. Persistencia PostgreSQL
```bash
# Ver estad√≠sticas de base de datos
curl http://localhost:8080/api/persistence/stats

# Obtener reportes guardados
curl "http://localhost:8080/api/persistence/reports?startDate=2024-10-01&endDate=2024-10-31"

# Top productos por revenue
curl "http://localhost:8080/api/persistence/products/top-revenue?limit=10"
```

**Operaciones:** Escritura JDBC (Spark ‚Üí PostgreSQL) y lectura JPA (PostgreSQL ‚Üí Spring Boot)

---

### üîÑ Bloque 4: Batch Processing y Automatizaci√≥n ‚ú® NUEVO

#### 1. Dashboard de Batch Jobs
```bash
# Ver dashboard con estad√≠sticas de todos los jobs
curl http://localhost:8080/api/batch/dashboard
```

**Respuesta:**
```json
{
    "totalExecutions": 7,
    "successfulExecutions": 7,
    "failedExecutions": 0,
    "lastExecution": {
        "id": 7,
        "jobName": "INCREMENTAL_PROCESSING",
        "status": "SUCCESS",
        "recordsProcessed": 0,
        "durationMs": 292
    },
    "jobNames": ["ETL_DAILY_PIPELINE", "INCREMENTAL_PROCESSING"]
}
```

#### 2. Ejecutar ETL Pipeline
```bash
# Ejecutar pipeline completo ETL
curl -X POST http://localhost:8080/api/batch/etl/run
```

**ETL Pipeline incluye:**
- Extract: Lectura de CSVs (transactions, products)
- Transform: Limpieza, validaci√≥n, enriquecimiento, agregaci√≥n
- Load: Persistencia a PostgreSQL + Parquet

#### 3. Procesamiento Incremental
```bash
# Procesar solo datos nuevos desde fecha espec√≠fica
curl -X POST "http://localhost:8080/api/batch/incremental/run?since=2025-01-01T00:00:00"
```

**Operaci√≥n:** Filtra transacciones por `transaction_date >= since` y las procesa

#### 4. Generar Reporte Diario
```bash
# Generar reporte autom√°tico para una fecha espec√≠fica
curl -X POST "http://localhost:8080/api/batch/report/generate?reportDate=2025-01-15"
```

**Reporte incluye:**
- Total de ventas del d√≠a
- N√∫mero de transacciones
- Clientes √∫nicos
- Ticket promedio
- Categor√≠a y producto m√°s vendido
- Alertas de fraude detectadas

#### 5. Historial de Ejecuciones
```bash
# Ver todas las ejecuciones
curl http://localhost:8080/api/batch/executions

# Filtrar por nombre de job
curl "http://localhost:8080/api/batch/executions?jobName=ETL_DAILY_PIPELINE"

# Filtrar por status
curl "http://localhost:8080/api/batch/executions?status=SUCCESS"
```

**Respuesta:**
```json
[
    {
        "id": 3,
        "jobName": "ETL_DAILY_PIPELINE",
        "status": "SUCCESS",
        "recordsProcessed": 20,
        "recordsFailed": 0,
        "durationMs": 1072,
        "startTime": "2025-10-03T15:00:11",
        "endTime": "2025-10-03T15:00:12"
    }
]
```

#### 6. Consultar Reportes por Rango de Fechas
```bash
# Obtener reportes generados en un rango de fechas
curl "http://localhost:8080/api/batch/reports?startDate=2025-01-01&endDate=2025-01-31"
```

**Respuesta:**
```json
[
    {
        "id": 2,
        "reportDate": "2025-01-15",
        "totalSales": 4049.61,
        "totalTransactions": 20,
        "uniqueCustomers": 17,
        "avgTicket": 202.48,
        "topCategory": "Electronics",
        "topProduct": "Gaming Chair",
        "fraudAlertsCount": 3,
        "generatedAt": "2025-10-03T16:45:00"
    }
]
```

#### 7. M√©tricas por Job Name
```bash
# Obtener m√©tricas agregadas de un job espec√≠fico
curl http://localhost:8080/api/batch/metrics/ETL_DAILY_PIPELINE
```

**Operaci√≥n:** Calcula promedio de duraci√≥n, total de registros procesados, tasa de √©xito

#### 8. Jobs Programados Autom√°ticos

El sistema incluye 4 jobs automatizados con Spring @Scheduled:

| Job | Frecuencia | Descripci√≥n |
|-----|------------|-------------|
| **Daily ETL Pipeline** | Diario a las 2:00 AM | Ejecuta el ETL completo de todos los datos |
| **Daily Report Generation** | Diario a las 3:00 AM | Genera reporte autom√°tico del d√≠a anterior |
| **Hourly Incremental Processing** | Cada hora | Procesa solo datos de la √∫ltima hora |
| **System Health Check** | Cada 15 minutos | Verifica estado del sistema |

**Configuraci√≥n:**
```yaml
# application-prod.yml
scheduling:
  enabled: true  # Activar jobs en producci√≥n

# application-dev.yml
scheduling:
  enabled: false  # Desactivar jobs en desarrollo
```

---

## üìä Datasets de Ejemplo

Ubicaci√≥n: `./data/`

- **transactions.csv**: 20 transacciones de ejemplo
- **products.csv**: 17 productos con categor√≠as
- **customers.csv**: 17 clientes con informaci√≥n demogr√°fica

---

## üèóÔ∏è Estructura del Proyecto

```
ecommerce-analytics/
‚îú‚îÄ‚îÄ docker-compose.yml              # Infraestructura Spark + PostgreSQL
‚îú‚îÄ‚îÄ pom.xml                         # Dependencias Maven
‚îú‚îÄ‚îÄ start.sh                        # Script de inicio automatizado
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ transactions.csv            # Dataset transacciones
‚îÇ   ‚îú‚îÄ‚îÄ products.csv                # Dataset productos
‚îÇ   ‚îî‚îÄ‚îÄ customers.csv               # Dataset clientes
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ BLOQUE1_GUIA_COMPLETA.md    # Gu√≠a completa Bloque 1
‚îÇ   ‚îú‚îÄ‚îÄ BLOQUE2_GUIA_COMPLETA.md    # Gu√≠a completa Bloque 2
‚îÇ   ‚îú‚îÄ‚îÄ BLOQUE3_GUIA_COMPLETA.md    # Gu√≠a completa Bloque 3
‚îÇ   ‚îî‚îÄ‚îÄ BLOQUE4_GUIA_COMPLETA.md    # Gu√≠a completa Bloque 4 ‚ú® NUEVO
‚îú‚îÄ‚îÄ BLOQUE4_RESUMEN.md              # Resumen ejecutivo Bloque 4 ‚ú® NUEVO
‚îú‚îÄ‚îÄ RESULTADOS_PRUEBAS.md           # Resultados de tests completos ‚ú® NUEVO
‚îú‚îÄ‚îÄ test_all_blocks.sh              # Script de pruebas autom√°ticas ‚ú® NUEVO
‚îú‚îÄ‚îÄ src/main/java/com/ecommerce/analytics/
‚îÇ   ‚îú‚îÄ‚îÄ EcommerceAnalyticsApplication.java    # Main class
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SparkConfig.java                  # Configuraci√≥n SparkSession
‚îÇ   ‚îú‚îÄ‚îÄ udf/                                  # ‚ú® NUEVO - Bloque 3
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CustomUDFs.java                   # 5 UDFs personalizadas
‚îÇ   ‚îú‚îÄ‚îÄ entity/                               # ‚ú® Bloque 3 & 4
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SalesReportEntity.java            # Entidad JPA reportes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FraudAlertEntity.java             # Entidad JPA alertas fraude
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ProductPerformanceEntity.java     # Entidad JPA m√©tricas productos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BatchJobExecutionEntity.java      # Entidad JPA tracking jobs ‚ú® NUEVO
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DailyReportEntity.java            # Entidad JPA reportes diarios ‚ú® NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ repository/                           # ‚ú® Bloque 3 & 4
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SalesReportRepository.java        # Repositorio Spring Data JPA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FraudAlertRepository.java         # Repositorio alertas fraude
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ProductPerformanceRepository.java # Repositorio m√©tricas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BatchJobExecutionRepository.java  # Repositorio tracking jobs ‚ú® NUEVO
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DailyReportRepository.java        # Repositorio reportes diarios ‚ú® NUEVO
‚îÇ   ‚îú‚îÄ‚îÄ model/                                # DTOs Bloque 2
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SalesByCategory.java              # DTO ventas por categor√≠a
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TopProduct.java                   # DTO productos m√°s vendidos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DailySalesSummary.java            # DTO resumen diario
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ SalesByRegion.java                # DTO ventas por regi√≥n
‚îÇ   ‚îú‚îÄ‚îÄ service/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DataReaderService.java            # Servicio lectura de datos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AnalyticsService.java             # An√°lisis y agregaciones (Bloque 2)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ OptimizationService.java          # Cach√© y optimizaci√≥n (Bloque 3)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FraudDetectionService.java        # Detecci√≥n de fraude (Bloque 3)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ PersistenceService.java           # Persistencia PostgreSQL (Bloque 3)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BatchJobService.java              # ETL Pipeline ‚ú® NUEVO (Bloque 4)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ReportService.java                # Generaci√≥n reportes ‚ú® NUEVO (Bloque 4)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ BatchJobScheduler.java            # Jobs programados ‚ú® NUEVO (Bloque 4)
‚îÇ   ‚îî‚îÄ‚îÄ controller/
‚îÇ       ‚îú‚îÄ‚îÄ DataExplorationController.java    # REST endpoints Bloque 1
‚îÇ       ‚îú‚îÄ‚îÄ SalesAnalyticsController.java     # Endpoints ventas (Bloque 2)
‚îÇ       ‚îú‚îÄ‚îÄ ProductAnalyticsController.java   # Endpoints productos (Bloque 2)
‚îÇ       ‚îú‚îÄ‚îÄ OptimizationController.java       # Endpoints optimizaci√≥n (Bloque 3)
‚îÇ       ‚îú‚îÄ‚îÄ FraudDetectionController.java     # Endpoints fraude (Bloque 3)
‚îÇ       ‚îú‚îÄ‚îÄ PersistenceController.java        # Endpoints persistencia (Bloque 3)
‚îÇ       ‚îî‚îÄ‚îÄ BatchJobController.java           # Endpoints batch jobs ‚ú® NUEVO (Bloque 4)
‚îî‚îÄ‚îÄ src/main/resources/
    ‚îú‚îÄ‚îÄ application.yml               # Configuraci√≥n base
    ‚îú‚îÄ‚îÄ application-local.yml         # Perfil local
    ‚îî‚îÄ‚îÄ application-docker.yml        # Perfil Docker
```

---

## üîë Conceptos Clave Implementados

### Bloque 1: Fundamentos

#### 1. SparkSession como Bean de Spring
- Configuraci√≥n en `SparkConfig.java`
- Perfiles separados: `local` y `docker`
- Inyecci√≥n de dependencias con `@Autowired`

#### 2. Lectura de Datos CSV
- Headers autom√°ticos con `option("header", "true")`
- Inferencia de schema con `option("inferSchema", "true")`
- DataFrames tipados como `Dataset<Row>`

#### 3. Operaciones B√°sicas de Spark
- `count()`: Contar registros (acci√≥n)
- `show()`: Mostrar datos (acci√≥n)
- `printSchema()`: Ver estructura de datos
- **Lazy Evaluation**: Las transformaciones no se ejecutan hasta una acci√≥n

#### 4. Integraci√≥n Spring Boot + Spark
- Servicio `DataReaderService` con inyecci√≥n de SparkSession
- Endpoints REST que exponen resultados de Spark
- Conversi√≥n de DataFrames a JSON para APIs

### Bloque 2: Transformaciones y An√°lisis

#### 1. Transformaciones Avanzadas
- `select()`: Selecci√≥n de columnas
- `filter()`: Filtrado de datos
- `orderBy()`: Ordenamiento con `asc()` y `desc()`
- Alias de columnas con `alias()`

#### 2. Agregaciones de Negocio
- `groupBy()`: Agrupaci√≥n por una o m√°s columnas
- `agg()`: M√∫ltiples agregaciones en una operaci√≥n
- Funciones: `sum()`, `avg()`, `count()`, `countDistinct()`, `max()`, `min()`

#### 3. Joins entre Datasets
- `join()`: Inner join por defecto
- Join entre `transactions`, `products` y `customers`
- Enriquecimiento de datos con informaci√≥n relacionada

#### 4. Window Functions
- `Window.orderBy()`: Definici√≥n de ventana
- `row_number()`: Ranking de productos
- Uso de `.over(windowSpec)` para aplicar funci√≥n

#### 5. Filtrado Din√°mico
- Filtros opcionales con `@RequestParam(required = false)`
- Filtrado por rango de fechas con `.between()`
- Funciones de fecha: `to_date()`, `date_format()`

#### 6. Conversi√≥n Dataset<Row> a DTOs
- Uso de `collectAsList()` para materializar resultados
- Streams de Java para mapear `Row` a POJOs
- Lombok para reducir boilerplate en DTOs

### Bloque 3: Procesamiento Avanzado y Optimizaci√≥n ‚ú® NUEVO

#### 1. User Defined Functions (UDFs)
- 5 UDFs personalizadas implementadas como clases `Serializable`
- `ValidateEmail`: Validaci√≥n de emails con regex
- `CategorizeAmount`: Clasificaci√≥n de montos (BAJO/MEDIO/ALTO/MUY_ALTO)
- `DetectFraud`: Detecci√≥n b√°sica de fraude (BAJO/MEDIO/ALTO_RIESGO)
- `NormalizeString`: Normalizaci√≥n de texto (uppercase, trim)
- `CalculateDiscount`: C√°lculo de descuentos progresivos
- Registro din√°mico de UDFs con `spark.udf().register()`

#### 2. Optimizaci√≥n con Cach√©
- `persist(StorageLevel.MEMORY_AND_DISK)`: Cach√© h√≠brido
- Gesti√≥n de ciclo de vida del cach√© (cache/unpersist)
- Mejora de performance: ~10x m√°s r√°pido en queries repetitivos
- Monitoreo de datasets cacheados

#### 3. Broadcast Joins
- Optimizaci√≥n de joins con tablas peque√±as usando `broadcast()`
- Reducci√≥n de shuffle en el cluster
- Mejora significativa en performance para joins dimensionales

#### 4. Detecci√≥n de Fraude con Machine Learning B√°sico
- **Algoritmo**: Detecci√≥n de outliers usando Z-score (desviaci√≥n est√°ndar)
- **Criterios**:
  - Monto > umbral de desviaciones est√°ndar (configurable)
  - Cantidad excesiva de unidades (> 10)
  - Combinaci√≥n de factores de riesgo
- **An√°lisis de patrones**:
  - Agrupaci√≥n por cliente
  - Agrupaci√≥n por producto
  - Detecci√≥n de duplicados sospechosos

#### 5. Persistencia Bidireccional PostgreSQL
- **Escritura (Spark ‚Üí PostgreSQL)**:
  - JDBC con `df.write().format("jdbc")`
  - SaveMode configurable (Append, Overwrite, ErrorIfExists)
- **Lectura (PostgreSQL ‚Üí Spring Boot)**:
  - Spring Data JPA con repositorios
  - Queries personalizadas con @Query
- **Entidades JPA**:
  - SalesReportEntity: Reportes de ventas agregados
  - FraudAlertEntity: Alertas de fraude detectadas
  - ProductPerformanceEntity: M√©tricas de productos
- **Conversi√≥n bidireccional**: Dataset<Row> ‚Üî Entity

#### 6. Limpieza y Validaci√≥n de Datos
- Eliminaci√≥n de nulls con `na.drop()`
- Eliminaci√≥n de duplicados con `dropDuplicates()`
- Validaci√≥n de datos con UDFs antes del procesamiento

### Bloque 4: Batch Processing y Automatizaci√≥n ‚ú® NUEVO

#### 1. ETL Pipeline Completo
- **Extract**: Lectura de m√∫ltiples fuentes de datos (CSVs)
- **Transform**: Limpieza, validaci√≥n, enriquecimiento, agregaci√≥n en cadena
- **Load**: Dual storage (PostgreSQL + Parquet files)
- **Tracking**: Registro completo de m√©tricas de ejecuci√≥n (duraci√≥n, registros procesados/fallidos)
- Manejo de errores con try-catch y registro de fallas

#### 2. Procesamiento Incremental
- Filtrado por fecha desde √∫ltima ejecuci√≥n
- Procesamiento eficiente de solo datos nuevos
- Optimizaci√≥n de recursos al evitar reprocesamiento

#### 3. Jobs Programados con Spring @Scheduled
- **Cron Expressions**: Configuraci√≥n flexible de horarios
  - Daily: `"0 0 2 * * *"` (2:00 AM)
  - Hourly: `"0 0 * * * *"` (cada hora)
  - Fixed Rate: `fixedRate = 900000` (15 minutos)
- **@EnableScheduling**: Activaci√≥n de scheduling
- **Conditional Scheduling**: Control por perfil (enabled/disabled en dev/prod)

#### 4. Generaci√≥n Autom√°tica de Reportes
- Reportes diarios con m√©tricas de negocio
- C√°lculo de KPIs: ventas totales, ticket promedio, top productos/categor√≠as
- Almacenamiento en PostgreSQL para hist√≥rico
- Integraci√≥n con detecci√≥n de fraude

#### 5. Sistema de Tracking de Jobs
- Entidad `BatchJobExecutionEntity` con:
  - Estado del job (RUNNING/SUCCESS/FAILED)
  - Timestamp de inicio y fin
  - Duraci√≥n en milisegundos
  - Registros procesados y fallidos
  - Mensaje de error si aplica
- Dashboard con m√©tricas agregadas
- Filtros por job name y status

#### 6. Dual Storage Pattern
- **PostgreSQL**: Datos transaccionales y reportes (OLTP)
- **Parquet**: Analytics y procesamiento masivo (OLAP)
- Escritura paralela a ambos destinos
- SaveMode configurable (Append/Overwrite)

#### 7. Configuraci√≥n Multi-Ambiente
- **application-dev.yml**: Scheduling deshabilitado, logs DEBUG
- **application-prod.yml**: Scheduling habilitado, optimizaciones, logs INFO
- Variables de entorno para credenciales sensibles
- Pool de conexiones optimizado por ambiente

---

## üéØ Ejercicios Pr√°cticos del Bloque 1

### Ejercicio 1: Explorar Productos
```bash
# Ver primeras 20 filas de productos
curl http://localhost:8080/api/data/products?limit=20

# Observar el schema inferido
# Contar total de productos
```

### Ejercicio 2: Entender Lazy Evaluation
Agregar logging en `DataReaderService.java` para ver cu√°ndo se ejecutan las operaciones:
```java
System.out.println("Leyendo CSV..."); // Se ejecuta inmediatamente
Dataset<Row> df = sparkSession.read()...
System.out.println("DataFrame creado"); // A√∫n no se ley√≥ el archivo

long count = df.count(); // AQU√ç se ejecuta la lectura
System.out.println("Archivo le√≠do y contado: " + count);
```

---

## üìù Configuraci√≥n de Perfiles

### Perfil Local (`application-local.yml`)
- Spark Master: `local[*]` (todos los cores locales)
- PostgreSQL: `localhost:5432`
- Path de datos: `./data`

### Perfil Docker (`application-docker.yml`)
- Spark Master: `spark://spark-master:7077`
- PostgreSQL: `postgres:5432`
- Path de datos: `/data`

---

## üê≥ Comandos √ötiles de Docker

```bash
# Ver logs del cluster Spark
docker logs spark-master
docker logs spark-worker-1

# Detener servicios
docker-compose down

# Reiniciar servicios
docker-compose restart

# Eliminar todo (incluyendo vol√∫menes)
docker-compose down -v
```

---

## ‚úÖ Checklist de Verificaci√≥n Bloque 1

- [x] Proyecto Spring Boot creado con Maven
- [x] Dependencias de Spark agregadas
- [x] Docker Compose configurado (Spark + PostgreSQL)
- [x] SparkSession configurado como Bean
- [x] Perfiles `local` y `docker` funcionando
- [x] Datasets CSV creados
- [x] Servicio de lectura de datos implementado
- [x] Endpoints REST expuestos
- [x] Health check funcionando
- [x] Primera lectura exitosa de datos

---

## üéì Conceptos Aprendidos

1. **Arquitectura Spark**: Driver, Executors, Cluster Manager
2. **RDD vs DataFrame vs Dataset**: Diferencias y cu√°ndo usar cada uno
3. **Lazy Evaluation**: Transformaciones vs Acciones
4. **SparkSession**: Punto de entrada para trabajar con Spark
5. **Integraci√≥n Spring Boot**: Inyecci√≥n de dependencias y configuraci√≥n por perfiles

---

## üéâ Proyecto Completado

**Todos los 4 bloques han sido implementados y verificados exitosamente:**

‚úÖ **Bloque 1**: Fundamentos y configuraci√≥n - Lectura de datos, API REST b√°sica
‚úÖ **Bloque 2**: Transformaciones y an√°lisis - Agregaciones, joins, window functions
‚úÖ **Bloque 3**: Procesamiento avanzado - UDFs, optimizaci√≥n, detecci√≥n de fraude, persistencia
‚úÖ **Bloque 4**: Batch processing y automatizaci√≥n - ETL pipeline, jobs programados, reportes autom√°ticos

**Estado del Proyecto**: ‚úÖ PRODUCTION READY

**Resultados de Tests**:
- Total de tests ejecutados: 31
- Block 4 success rate: 100% (8/8 tests passed)
- ETL Pipeline: 20 registros procesados en 1.072s
- Scheduled jobs: Funcionando correctamente (hourly incremental processing verificado)

Consulta `RESULTADOS_PRUEBAS.md` para el an√°lisis completo de pruebas.

---

## üÜò Troubleshooting

Para problemas de compatibilidad y errores comunes, consulta la **gu√≠a completa de troubleshooting** en:
üìñ `docs/CONFIGURACION_CORRECTA.md`

### Errores Comunes:

#### ‚ùå Java 18+ Incompatibilidad
```
Error: getSubject is supported only if a security manager is allowed
```
**Soluci√≥n**: Usar Java 17. Ver instrucciones de instalaci√≥n arriba.

#### ‚ùå ANTLR Version Conflict (CR√çTICO)
```
Error: Could not deserialize ATN with version 4 (expected 3)
```
**Causa**: Spring Boot 3.x usa Hibernate con ANTLR 4.10+, incompatible con Spark 3.5.0 que requiere ANTLR 4.9.3.

**Soluci√≥n Aplicada**:
1. **Downgrade Spring Boot**: 3.2.0 ‚Üí 2.7.18
2. **Imports JPA**: `jakarta.persistence.*` ‚Üí `javax.persistence.*`
3. **Exclusiones ANTLR**: Agregadas en `pom.xml` para `spring-boot-starter-data-jpa`
4. **Versi√≥n forzada**: ANTLR 4.9.3 en `<dependencyManagement>`

**Verificaci√≥n**:
```bash
mvn clean compile
# Debe compilar sin errores
```

#### ‚ùå Puerto 8080 en uso
```bash
# Matar proceso en puerto 8080
lsof -ti:8080 | xargs kill -9

# O cambiar puerto en application.yml
server:
  port: 8081
```

#### ‚ùå PostgreSQL no conecta
```bash
# Verificar contenedor
docker ps | grep postgres

# Reiniciar PostgreSQL
docker-compose restart postgres

# Ver logs
docker logs postgres-db
```

#### ‚ùå Module Access Errors (Java 17)
**Error:**
```
IllegalAccessError: class org.apache.spark.storage.StorageUtils$ cannot access
class sun.nio.ch.DirectBuffer
```

**Soluci√≥n:**
- Si ejecutas con Maven: Los JVM arguments se aplican autom√°ticamente desde `pom.xml`
- Si ejecutas el JAR: Debes agregar los argumentos `--add-opens` manualmente (ver comando arriba)
- Soluci√≥n r√°pida: Usa el script `./start.sh`

---

## üìö Recursos Adicionales

### Documentaci√≥n del Proyecto
- üìñ **PRD Completo**: `PRD.md` - Product Requirements Document
- üìò **Gu√≠a Bloque 1**: `docs/BLOQUE1_GUIA_COMPLETA.md` - Fundamentos y configuraci√≥n
- üìó **Gu√≠a Bloque 2**: `docs/BLOQUE2_GUIA_COMPLETA.md` - Transformaciones y an√°lisis
- üìô **Gu√≠a Bloque 3**: `docs/BLOQUE3_GUIA_COMPLETA.md` - Optimizaci√≥n y persistencia
- üìï **Gu√≠a Bloque 4**: `docs/BLOQUE4_GUIA_COMPLETA.md` - Batch processing y automatizaci√≥n ‚ú® NUEVO
- üìÑ **Resumen Bloque 4**: `BLOQUE4_RESUMEN.md` - Resumen ejecutivo ‚ú® NUEVO
- üìä **Resultados de Pruebas**: `RESULTADOS_PRUEBAS.md` - Tests y validaci√≥n completa ‚ú® NUEVO
- üß™ **Script de Tests**: `test_all_blocks.sh` - Suite de pruebas automatizada ‚ú® NUEVO
- üîß **Troubleshooting**: Ver secci√≥n "Consideraciones T√©cnicas Cr√≠ticas" y ANTLR conflict arriba

### Documentaci√≥n Oficial
- [Apache Spark 3.5.0 Documentation](https://spark.apache.org/docs/3.5.0/)
- [Spring Boot 2.7.18 Reference](https://docs.spring.io/spring-boot/docs/2.7.18/reference/html/)
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Spring Data JPA Reference](https://docs.spring.io/spring-data/jpa/docs/current/reference/html/)

---

**Versi√≥n:** 4.0 - Bloque 4 Completado - Batch Processing y Automatizaci√≥n ‚úÖ
**Fecha:** Octubre 2025
**√öltima Actualizaci√≥n:** Octubre 2025 - Bloque 4: ETL Pipeline, Jobs Programados, Reportes Autom√°ticos

## üìù Changelog

### **v4.0 - Bloque 4 Completado** (Actual) ‚ú®
- ‚úÖ **Bloque 4 Implementado y Verificado al 100%**: Batch processing y automatizaci√≥n completa
- ‚úÖ **ETL Pipeline completo**: Extract-Transform-Load con tracking de m√©tricas
- ‚úÖ **Procesamiento incremental**: Filtrado por fecha para procesar solo datos nuevos
- ‚úÖ **Jobs programados con @Scheduled**: 4 jobs automatizados (daily ETL, hourly incremental, reports, health check)
- ‚úÖ **Sistema de tracking de jobs**: BatchJobExecutionEntity con m√©tricas completas
- ‚úÖ **Generaci√≥n autom√°tica de reportes**: DailyReportEntity con KPIs de negocio
- ‚úÖ **Dual storage pattern**: PostgreSQL (OLTP) + Parquet (OLAP)
- ‚úÖ **Configuraci√≥n multi-ambiente**: application-dev.yml y application-prod.yml optimizados
- ‚úÖ **8 endpoints REST nuevos**: Dashboard, ETL, incremental, reportes, historial, m√©tricas
- ‚úÖ **2 entidades JPA nuevas**: BatchJobExecutionEntity, DailyReportEntity
- ‚úÖ **2 repositorios nuevos**: BatchJobExecutionRepository, DailyReportRepository
- ‚úÖ **3 servicios nuevos**: BatchJobService, ReportService, BatchJobScheduler
- ‚úÖ **Tests completos**: 31 tests ejecutados, Block 4 = 100% success (8/8)
- ‚úÖ **Documentaci√≥n completa**: BLOQUE4_RESUMEN.md, RESULTADOS_PRUEBAS.md, test_all_blocks.sh
- ‚úÖ **Verificaci√≥n con datos reales**: PostgreSQL conectado, Spark procesando, jobs ejecut√°ndose
- ‚úÖ **Estado**: PRODUCTION READY

### **v3.0 - Bloque 3 Completado**
- ‚úÖ **Bloque 3 Implementado y Verificado**: Procesamiento avanzado y optimizaci√≥n
- ‚úÖ 5 UDFs personalizadas (ValidateEmail, CategorizeAmount, DetectFraud, etc.)
- ‚úÖ Optimizaci√≥n con cach√© (persist) y broadcast joins
- ‚úÖ Sistema de detecci√≥n de fraude con an√°lisis estad√≠stico (Z-score)
- ‚úÖ Persistencia bidireccional PostgreSQL (Spark ‚Üî Spring Data JPA)
- ‚úÖ 3 entidades JPA (SalesReport, FraudAlert, ProductPerformance)
- ‚úÖ 3 repositorios Spring Data JPA con queries personalizadas
- ‚úÖ 20 endpoints REST nuevos (optimizaci√≥n, fraude, persistencia)
- ‚úÖ **Fix cr√≠tico ANTLR**: Spring Boot downgrade 3.2.0 ‚Üí 2.7.18
- ‚úÖ **Fix imports**: jakarta.persistence ‚Üí javax.persistence
- ‚úÖ **Fix UDF types**: Long ‚Üí Integer para quantity field (Spark compatibility)
- ‚úÖ **Fix timestamp casting**: Timestamp ‚Üí String para transactionDate
- ‚úÖ Documentaci√≥n completa en `BLOQUE3_GUIA_COMPLETA.md`
- ‚úÖ Todos los endpoints verificados y funcionando (24/24 endpoints)

### **v2.0 - Bloque 2 Completado**
- ‚úÖ **Bloque 2 Completado**: Transformaciones y an√°lisis de datos
- ‚úÖ 7 nuevos endpoints REST (ventas y productos)
- ‚úÖ Agregaciones avanzadas (groupBy, sum, avg, count, etc.)
- ‚úÖ Joins entre datasets (transactions + products + customers)
- ‚úÖ Window Functions para rankings
- ‚úÖ Filtrado din√°mico por fechas y categor√≠as
- ‚úÖ DTOs con Lombok para responses estructurados
- ‚úÖ Documentaci√≥n completa en `BLOQUE2_GUIA_COMPLETA.md`

### **v1.1 - Mejoras de Documentaci√≥n**
- ‚úÖ Agregada secci√≥n Quick Start con script automatizado (`start.sh`)
- ‚úÖ Comandos de ejecuci√≥n validados (Maven y JAR)
- ‚úÖ Documentaci√≥n de JVM arguments obligatorios para Java 17
- ‚úÖ Enlaces a documentaci√≥n completa y troubleshooting

### **v1.0 - Bloque 1 Inicial**
- ‚úÖ Setup inicial con Spark + Spring Boot
- ‚úÖ Lectura de CSVs y API REST b√°sica
