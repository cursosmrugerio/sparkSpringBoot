# ğŸ“š Bloque 1: Fundamentos de Spark y ConfiguraciÃ³n - GuÃ­a Completa

## ğŸ¯ Objetivo del Bloque 1

Aprender los fundamentos de Apache Spark y configurar un ambiente de desarrollo completo integrando Spark con Spring Boot, para realizar el primer procesamiento de datos.

---

## ğŸ“– Ãndice

1. [Â¿QuÃ© es Apache Spark?](#1-quÃ©-es-apache-spark)
2. [Arquitectura de Spark](#2-arquitectura-de-spark)
3. [Conceptos Clave](#3-conceptos-clave)
4. [ConfiguraciÃ³n del Proyecto](#4-configuraciÃ³n-del-proyecto)
5. [IntegraciÃ³n Spring Boot + Spark](#5-integraciÃ³n-spring-boot--spark)
6. [Primera Lectura de Datos](#6-primera-lectura-de-datos)
7. [API REST para Consultas](#7-api-rest-para-consultas)
8. [Troubleshooting](#8-troubleshooting)

---

## 1. Â¿QuÃ© es Apache Spark?

### DefiniciÃ³n

**Apache Spark** es un motor de procesamiento de datos distribuido y de alto rendimiento diseÃ±ado para analizar grandes volÃºmenes de datos de manera rÃ¡pida y eficiente.

### Â¿Por quÃ© Spark?

| CaracterÃ­stica | Procesamiento Tradicional | Apache Spark |
|---------------|---------------------------|--------------|
| **Velocidad** | Lento (disco) | 100x mÃ¡s rÃ¡pido (memoria) |
| **Escalabilidad** | Limitada | Distribuida en cluster |
| **Complejidad** | Alto cÃ³digo boilerplate | API simple y expresiva |
| **Lenguajes** | Generalmente uno | Java, Scala, Python, R, SQL |

### Casos de Uso Reales

- **E-commerce**: AnÃ¡lisis de comportamiento de compra, recomendaciones
- **Finanzas**: DetecciÃ³n de fraude en tiempo real
- **Telecomunicaciones**: AnÃ¡lisis de logs de red
- **Salud**: Procesamiento de datos mÃ©dicos masivos
- **IoT**: AnÃ¡lisis de datos de sensores

---

## 2. Arquitectura de Spark

### Componentes Principales

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SPARK APPLICATION                  â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         DRIVER PROGRAM                      â”‚    â”‚
â”‚  â”‚  - SparkContext/SparkSession                â”‚    â”‚
â”‚  â”‚  - PlanificaciÃ³n de tareas                  â”‚    â”‚
â”‚  â”‚  - CoordinaciÃ³n                             â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                â”‚                                      â”‚
â”‚                â”‚ Comunica con                         â”‚
â”‚                â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚         CLUSTER MANAGER                     â”‚    â”‚
â”‚  â”‚  - AsignaciÃ³n de recursos                   â”‚    â”‚
â”‚  â”‚  - GestiÃ³n de workers                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                â”‚                                      â”‚
â”‚                â”‚ Distribuye trabajo a                 â”‚
â”‚                â–¼                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            EXECUTORS (Workers)                â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚Task 1â”‚  â”‚Task 2â”‚  â”‚Task 3â”‚  â”‚Task 4â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚         Procesan datos en paralelo           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ExplicaciÃ³n de Componentes

#### 1. **Driver Program**
- **FunciÃ³n**: Cerebro de la aplicaciÃ³n Spark
- **Responsabilidades**:
  - Crear el `SparkSession` (punto de entrada)
  - Convertir cÃ³digo en plan de ejecuciÃ³n
  - Distribuir tareas a los executors
  - Recolectar resultados

#### 2. **Cluster Manager**
- **FunciÃ³n**: Gestor de recursos del cluster
- **Tipos**:
  - **Local**: Para desarrollo (1 mÃ¡quina)
  - **Standalone**: Cluster Spark nativo
  - **YARN**: Hadoop cluster manager
  - **Mesos**: Apache Mesos
  - **Kubernetes**: OrquestaciÃ³n de contenedores

#### 3. **Executors**
- **FunciÃ³n**: Workers que ejecutan tareas
- **Responsabilidades**:
  - Ejecutar cÃ³digo en paralelo
  - Almacenar datos en memoria/disco
  - Reportar estado al Driver

---

## 3. Conceptos Clave

### 3.1 RDD (Resilient Distributed Dataset)

**API de bajo nivel** (raramente usado hoy)

```java
// Ejemplo RDD (NO recomendado - solo para entender)
JavaRDD<String> lines = sparkContext.textFile("data.txt");
```

### 3.2 DataFrame

**API de alto nivel** (recomendado) - Similar a una tabla SQL

```java
// DataFrame: Datos estructurados con schema
Dataset<Row> df = sparkSession.read()
    .option("header", "true")
    .csv("transactions.csv");

df.show(5);  // Muestra primeras 5 filas
```

**Ventajas de DataFrames:**
- âœ… OptimizaciÃ³n automÃ¡tica de queries
- âœ… Schema conocido (tipos de datos)
- âœ… Compatible con SQL
- âœ… API similar a Pandas (Python) o dplyr (R)

### 3.3 Dataset<T>

DataFrame **tipado** (type-safe)

```java
// Dataset tipado con clase Transaction
Dataset<Transaction> transactions = df.as(Encoders.bean(Transaction.class));
```

### 3.4 Lazy Evaluation (EvaluaciÃ³n Perezosa)

**Concepto clave**: Spark NO ejecuta nada hasta que lo solicitas explÃ­citamente.

```java
// TRANSFORMACIONES (lazy - no ejecutan nada todavÃ­a)
Dataset<Row> filtered = df.filter("amount > 100");    // âŒ NO ejecuta
Dataset<Row> sorted = filtered.orderBy("amount");     // âŒ NO ejecuta

// ACCIONES (eager - ejecutan TODO el plan)
long count = sorted.count();                          // âœ… EJECUTA TODO
sorted.show();                                        // âœ… EJECUTA TODO
```

**Â¿Por quÃ© es importante?**
- Spark optimiza TODO el plan antes de ejecutar
- Evita cÃ¡lculos innecesarios
- Mejora drÃ¡sticamente el rendimiento

#### Tipos de Operaciones

| Transformaciones (Lazy) | Acciones (Eager) |
|-------------------------|------------------|
| `select()` | `count()` |
| `filter()` | `show()` |
| `groupBy()` | `collect()` |
| `orderBy()` | `save()` |
| `join()` | `take()` |

---

## 4. ConfiguraciÃ³n del Proyecto

### 4.1 Estructura del Proyecto

```
ecommerce-analytics/
â”œâ”€â”€ pom.xml                           # Dependencias Maven
â”œâ”€â”€ docker-compose.yml                # Infraestructura (Spark + PostgreSQL)
â”œâ”€â”€ README.md                         # DocumentaciÃ³n general
â”œâ”€â”€ data/                             # Datasets de ejemplo
â”‚   â”œâ”€â”€ transactions.csv              # 20 transacciones
â”‚   â”œâ”€â”€ products.csv                  # 17 productos
â”‚   â””â”€â”€ customers.csv                 # 17 clientes
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main/
â”‚       â”œâ”€â”€ java/com/ecommerce/analytics/
â”‚       â”‚   â”œâ”€â”€ EcommerceAnalyticsApplication.java    # Main class
â”‚       â”‚   â”œâ”€â”€ config/
â”‚       â”‚   â”‚   â””â”€â”€ SparkConfig.java                  # ConfiguraciÃ³n Spark
â”‚       â”‚   â”œâ”€â”€ service/
â”‚       â”‚   â”‚   â””â”€â”€ DataReaderService.java            # Lectura de datos
â”‚       â”‚   â””â”€â”€ controller/
â”‚       â”‚       â””â”€â”€ DataExplorationController.java    # REST endpoints
â”‚       â””â”€â”€ resources/
â”‚           â”œâ”€â”€ application.yml                       # Config base
â”‚           â”œâ”€â”€ application-local.yml                 # Config local
â”‚           â””â”€â”€ application-docker.yml                # Config Docker
â””â”€â”€ docs/
    â””â”€â”€ BLOQUE1_GUIA_COMPLETA.md                     # Este documento
```

### 4.2 Dependencias Maven (pom.xml)

```xml
<properties>
    <java.version>17</java.version>
    <spark.version>3.5.0</spark.version>
    <scala.binary.version>2.12</scala.binary.version>
</properties>

<dependencies>
    <!-- Spring Boot Web -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>

    <!-- Spring Boot Data JPA -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-jpa</artifactId>
    </dependency>

    <!-- Apache Spark Core -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-core_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
        <exclusions>
            <!-- Evitar conflictos de logging -->
            <exclusion>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-reload4j</artifactId>
            </exclusion>
        </exclusions>
    </dependency>

    <!-- Apache Spark SQL -->
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_${scala.binary.version}</artifactId>
        <version>${spark.version}</version>
    </dependency>

    <!-- PostgreSQL Driver -->
    <dependency>
        <groupId>org.postgresql</groupId>
        <artifactId>postgresql</artifactId>
        <scope>runtime</scope>
    </dependency>
</dependencies>
```

**ExplicaciÃ³n de versiones:**
- `spark-core`: Motor principal de Spark
- `spark-sql`: API de DataFrames y SQL
- `_2.12`: VersiÃ³n de Scala (Spark estÃ¡ escrito en Scala)
- `3.5.0`: VersiÃ³n de Spark

### 4.3 Docker Compose (Infraestructura)

```yaml
services:
  # Base de datos PostgreSQL
  postgres:
    image: postgres:15
    container_name: postgres-db
    environment:
      - POSTGRES_USER=sparkuser
      - POSTGRES_PASSWORD=sparkpass
      - POSTGRES_DB=ecommerce_analytics
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data

volumes:
  postgres-data:
```

**Â¿Por quÃ© PostgreSQL?**
- Almacenar resultados procesados por Spark
- Persistencia de reportes y agregaciones
- IntegraciÃ³n con Spring Data JPA

---

## 5. IntegraciÃ³n Spring Boot + Spark

### 5.1 ConfiguraciÃ³n de SparkSession

**Archivo**: `src/main/java/com/ecommerce/analytics/config/SparkConfig.java`

```java
@Configuration
public class SparkConfig {

    @Value("${spark.app.name}")
    private String appName;

    @Bean
    @Profile("local")
    public SparkSession sparkSessionLocal() {
        // ConfiguraciÃ³n para Java 17+
        System.setProperty("HADOOP_HOME", "/tmp/hadoop");
        System.setProperty("hadoop.home.dir", "/tmp/hadoop");

        return SparkSession.builder()
                .appName(appName)
                .master("local[*]")  // Usar todos los cores locales
                .config("spark.driver.host", "localhost")
                .config("spark.sql.shuffle.partitions", "4")
                .config("spark.hadoop.fs.defaultFS", "file:///")
                .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse")
                .getOrCreate();
    }
}
```

**Desglose de la configuraciÃ³n:**

| ConfiguraciÃ³n | Significado |
|---------------|-------------|
| `appName` | Nombre de la aplicaciÃ³n Spark |
| `master("local[*]")` | Modo local, usar todos los cores CPU |
| `spark.driver.host` | Host del driver (localhost para local) |
| `spark.sql.shuffle.partitions` | Particiones para shuffles (default: 200) |
| `spark.hadoop.fs.defaultFS` | Sistema de archivos (local) |
| `spark.sql.warehouse.dir` | Directorio para tablas temporales |

**Â¿Por quÃ© `@Profile("local")`?**
```java
@Profile("local")   // Se activa con: --spring.profiles.active=local
@Profile("docker")  // Se activa con: --spring.profiles.active=docker
```

Permite tener diferentes configuraciones segÃºn el ambiente.

### 5.2 Archivos de ConfiguraciÃ³n

#### application.yml (Base)
```yaml
spring:
  application:
    name: ecommerce-analytics
  profiles:
    active: local

spark:
  app:
    name: ${spring.application.name}

logging:
  level:
    com.ecommerce.analytics: INFO
    org.apache.spark: WARN
```

#### application-local.yml
```yaml
spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/ecommerce_analytics
    username: sparkuser
    password: sparkpass

spark:
  master: local[*]
  data:
    path: ./data  # Ruta relativa a los CSVs
```

#### application-docker.yml
```yaml
spring:
  datasource:
    url: jdbc:postgresql://postgres:5432/ecommerce_analytics
    username: sparkuser
    password: sparkpass

spark:
  master: spark://spark-master:7077  # Cluster Spark
  data:
    path: /data
```

---

## 6. Primera Lectura de Datos

### 6.1 Servicio de Lectura de Datos

**Archivo**: `src/main/java/com/ecommerce/analytics/service/DataReaderService.java`

```java
@Service
public class DataReaderService {

    @Autowired
    private SparkSession sparkSession;

    @Value("${spark.data.path}")
    private String dataPath;

    /**
     * Lee archivo CSV de transacciones
     * Demuestra: lectura bÃ¡sica, inferencia de schema, headers
     */
    public Dataset<Row> readTransactions() {
        String filePath = dataPath + "/transactions.csv";

        return sparkSession.read()
                .option("header", "true")       // Primera fila = nombres columnas
                .option("inferSchema", "true")  // Detectar tipos automÃ¡ticamente
                .csv(filePath);
    }

    /**
     * Lee archivo CSV de productos
     */
    public Dataset<Row> readProducts() {
        String filePath = dataPath + "/products.csv";

        return sparkSession.read()
                .option("header", "true")
                .option("inferSchema", "true")
                .csv(filePath);
    }

    /**
     * Muestra informaciÃ³n bÃ¡sica de un DataFrame
     */
    public void showDatasetInfo(Dataset<Row> dataset, String datasetName) {
        System.out.println("=== Dataset: " + datasetName + " ===");
        System.out.println("Total de registros: " + dataset.count());

        System.out.println("\nSchema:");
        dataset.printSchema();

        System.out.println("\nPrimeras 5 filas:");
        dataset.show(5);
    }
}
```

### 6.2 Operaciones BÃ¡sicas de Spark

#### OperaciÃ³n 1: Leer CSV

```java
Dataset<Row> df = sparkSession.read()
    .option("header", "true")       // Primera fila contiene nombres
    .option("inferSchema", "true")  // Detectar tipos (int, string, etc.)
    .csv("transactions.csv");
```

**Â¿QuÃ© hace `inferSchema`?**
```
transaction_id,amount,quantity
TXN001,59.98,2        â†’ amount: double, quantity: int
TXN002,129.99,1       â†’ Spark detecta tipos automÃ¡ticamente
```

#### OperaciÃ³n 2: Ver Schema

```java
df.printSchema();
```

**Output:**
```
root
 |-- transaction_id: string (nullable = true)
 |-- customer_id: string (nullable = true)
 |-- product_id: string (nullable = true)
 |-- quantity: integer (nullable = true)
 |-- amount: double (nullable = true)
 |-- transaction_date: timestamp (nullable = true)
 |-- payment_method: string (nullable = true)
 |-- region: string (nullable = true)
```

#### OperaciÃ³n 3: Mostrar Datos

```java
df.show(5);  // Muestra 5 filas
```

**Output:**
```
+--------------+-----------+----------+--------+------+-------------------+--------------+------+
|transaction_id|customer_id|product_id|quantity|amount|   transaction_date|payment_method|region|
+--------------+-----------+----------+--------+------+-------------------+--------------+------+
|        TXN001|   CUST101|  PROD001 |       2| 59.98|2024-10-01 10:30:00|   credit_card| North|
|        TXN002|   CUST102|  PROD002 |       1|129.99|2024-10-01 11:15:00|    debit_card| South|
+--------------+-----------+----------+--------+------+-------------------+--------------+------+
```

#### OperaciÃ³n 4: Contar Registros

```java
long count = df.count();  // ACCIÃ“N - Ejecuta todo el plan
System.out.println("Total registros: " + count);
```

---

## 7. API REST para Consultas

### 7.1 Controlador REST

**Archivo**: `src/main/java/com/ecommerce/analytics/controller/DataExplorationController.java`

```java
@RestController
@RequestMapping("/api/data")
public class DataExplorationController {

    @Autowired
    private DataReaderService dataReaderService;

    /**
     * GET /api/data/transactions?limit=10
     * Retorna transacciones en formato JSON
     */
    @GetMapping("/transactions")
    public ResponseEntity<Map<String, Object>> getTransactions(
            @RequestParam(defaultValue = "10") int limit) {

        Dataset<Row> transactions = dataReaderService.readTransactions();

        Map<String, Object> response = new HashMap<>();
        response.put("totalRecords", transactions.count());
        response.put("schema", getSchemaInfo(transactions));
        response.put("data", getDataAsList(transactions, limit));

        return ResponseEntity.ok(response);
    }

    /**
     * GET /api/data/health
     * Health check del sistema Spark
     */
    @GetMapping("/health")
    public ResponseEntity<Map<String, String>> healthCheck() {
        Map<String, String> response = new HashMap<>();
        response.put("status", "OK");
        response.put("spark", "Running");
        response.put("message", "Spark integration is working correctly");

        return ResponseEntity.ok(response);
    }

    // MÃ©todos auxiliares para convertir DataFrame a JSON
    private List<String> getSchemaInfo(Dataset<Row> dataset) {
        return List.of(dataset.schema().fields()).stream()
                .map(field -> field.name() + ": " + field.dataType().simpleString())
                .collect(Collectors.toList());
    }

    private List<Map<String, Object>> getDataAsList(Dataset<Row> dataset, int limit) {
        return dataset.limit(limit)
                .collectAsList()  // ACCIÃ“N - trae datos al driver
                .stream()
                .map(row -> {
                    Map<String, Object> map = new HashMap<>();
                    for (int i = 0; i < row.size(); i++) {
                        map.put(row.schema().fields()[i].name(), row.get(i));
                    }
                    return map;
                })
                .collect(Collectors.toList());
    }
}
```

### 7.2 Endpoints Disponibles

#### 1. Health Check
```bash
curl http://localhost:8080/api/data/health
```

**Respuesta:**
```json
{
  "status": "OK",
  "spark": "Running",
  "message": "Spark integration is working correctly"
}
```

#### 2. Listar Transacciones
```bash
curl http://localhost:8080/api/data/transactions?limit=5
```

**Respuesta:**
```json
{
  "totalRecords": 20,
  "schema": [
    "transaction_id: string",
    "customer_id: string",
    "product_id: string",
    "quantity: int",
    "amount: double"
  ],
  "data": [
    {
      "transaction_id": "TXN001",
      "customer_id": "CUST101",
      "product_id": "PROD001",
      "quantity": 2,
      "amount": 59.98
    }
  ]
}
```

#### 3. Listar Productos
```bash
curl http://localhost:8080/api/data/products?limit=5
```

#### 4. Listar Clientes
```bash
curl http://localhost:8080/api/data/customers?limit=5
```

---

## 8. Troubleshooting

### Problema 1: Error de Java Version

**Error:**
```
getSubject is supported only if a security manager is allowed
```

**Causa:** Spark 3.5.0 no es compatible con Java 18+

**SoluciÃ³n:**
```bash
# Instalar Java 17
sdk install java 17.0.9-tem

# Verificar versiÃ³n
java -version  # Debe mostrar 17.x.x

# Ejecutar con Java 17
java -jar target/analytics-1.0.0.jar --spring.profiles.active=local
```

### Problema 2: Puerto 8080 en Uso

**Error:**
```
Port 8080 is already in use
```

**SoluciÃ³n:**
```yaml
# En application.yml
server:
  port: 8081
```

### Problema 3: No Encuentra Archivos CSV

**Error:**
```
FileNotFoundException: ./data/transactions.csv
```

**SoluciÃ³n:**
```bash
# Verificar que existe el directorio
ls -la data/

# Verificar ruta en application-local.yml
spark:
  data:
    path: ./data  # Ruta relativa desde donde se ejecuta la app
```

### Problema 4: PostgreSQL No Conecta

**Error:**
```
Connection refused: localhost:5432
```

**SoluciÃ³n:**
```bash
# Verificar que PostgreSQL estÃ¡ corriendo
docker ps | grep postgres

# Si no estÃ¡ corriendo
docker compose up -d postgres

# Verificar logs
docker logs postgres-db
```

---

## ğŸ“ Ejercicios PrÃ¡cticos

### Ejercicio 1: Explorar el Dataset

**Objetivo:** Familiarizarse con las operaciones bÃ¡sicas de Spark

```java
// 1. Leer dataset de productos
Dataset<Row> products = dataReaderService.readProducts();

// 2. Mostrar schema
products.printSchema();

// 3. Contar productos
long totalProducts = products.count();

// 4. Mostrar primeros 10
products.show(10);

// 5. Filtrar productos de categorÃ­a "Electronics"
Dataset<Row> electronics = products.filter("category = 'Electronics'");
electronics.show();
```

### Ejercicio 2: Crear Nuevo Endpoint

**Objetivo:** Crear endpoint para buscar productos por categorÃ­a

```java
@GetMapping("/products/category/{category}")
public ResponseEntity<List<Map<String, Object>>> getProductsByCategory(
        @PathVariable String category) {

    Dataset<Row> products = dataReaderService.readProducts();
    Dataset<Row> filtered = products.filter(col("category").equalTo(category));

    List<Map<String, Object>> result = getDataAsList(filtered, 100);
    return ResponseEntity.ok(result);
}
```

**Probar:**
```bash
curl http://localhost:8080/api/data/products/category/Electronics
```

### Ejercicio 3: Agregar Logging

**Objetivo:** Entender lazy evaluation

```java
public Dataset<Row> readTransactions() {
    logger.info("Leyendo CSV..."); // Se ejecuta inmediatamente

    Dataset<Row> df = sparkSession.read()
        .option("header", "true")
        .csv(dataPath + "/transactions.csv");

    logger.info("DataFrame creado"); // Se ejecuta inmediatamente
    // PERO aÃºn NO se ha leÃ­do el archivo

    long count = df.count(); // AQUÃ se ejecuta la lectura
    logger.info("Archivo leÃ­do, {} registros", count);

    return df;
}
```

---

## ğŸ“ Conceptos Clave Aprendidos

### âœ… Checklist de Aprendizaje

- [ ] Entiendo quÃ© es Apache Spark y sus ventajas
- [ ] Conozco la arquitectura Driver-Executor
- [ ] Comprendo la diferencia entre RDD, DataFrame y Dataset
- [ ] Entiendo Lazy Evaluation (Transformaciones vs Acciones)
- [ ] SÃ© configurar SparkSession en Spring Boot
- [ ] Puedo leer archivos CSV con Spark
- [ ] Conozco operaciones bÃ¡sicas: count(), show(), printSchema()
- [ ] SÃ© exponer datos de Spark mediante REST API
- [ ] Entiendo perfiles de configuraciÃ³n (local vs docker)

### ğŸ”‘ Puntos Clave para Recordar

1. **Spark es Lazy**: No ejecuta nada hasta que haces `.count()`, `.show()`, etc.
2. **DataFrames son inmutables**: Cada transformaciÃ³n crea un nuevo DataFrame
3. **SparkSession es el punto de entrada**: Todo comienza con `sparkSession.read()`
4. **Perfiles de Spring Boot**: Permiten configuraciones diferentes por ambiente
5. **CSV Options importantes**:
   - `header=true`: Primera fila son nombres de columnas
   - `inferSchema=true`: Detecta tipos automÃ¡ticamente

---

## âš ï¸ CONSIDERACIONES TÃ‰CNICAS CRÃTICAS

Esta secciÃ³n documenta problemas de compatibilidad conocidos y sus soluciones validadas. **Es fundamental seguir estas instrucciones para evitar errores.**

### ğŸ”´ Compatibilidad de Versiones

#### Java 17 - Requisito Estricto

- âœ… **Java 17**: Compatible y validado (usar 17.0.13)
- âŒ **Java 18+**: **INCOMPATIBLE** - Spark 3.5.0 usa Hadoop que requiere Security Manager (removido en Java 18+)

**Error tÃ­pico con Java 18+:**
```
getSubject is supported only if a security manager is allowed
```

**SoluciÃ³n:**
```bash
# Instalar Java 17 con SDKMAN
sdk install java 17.0.13-tem
sdk use java 17.0.13-tem

# Verificar (debe mostrar 17.0.13)
java -version
```

#### Conflicto ANTLR

- **Problema**: Spring Boot 3.2.0 incluye ANTLR 4.13.1, pero Spark 3.5.0 requiere ANTLR 4.9.x
- **SÃ­ntoma**: `Could not deserialize ATN with version 3 (expected 4)`

**SoluciÃ³n (ya implementada en pom.xml):**
```xml
<dependencyManagement>
    <dependencies>
        <dependency>
            <groupId>org.antlr</groupId>
            <artifactId>antlr4-runtime</artifactId>
            <version>4.9.3</version>
        </dependency>
    </dependencies>
</dependencyManagement>
```

#### Conflicto Jakarta EE vs javax

- **Problema**: Spring Boot 3 usa Jakarta EE (`jakarta.servlet.*`), Spark usa javax (`javax.servlet.*`)
- **SÃ­ntoma**: `NoClassDefFoundError: javax/servlet/Servlet`

**SoluciÃ³n (ya implementada en pom.xml):**
```xml
<!-- Javax Servlet API (requerido por Spark 3.5.0) -->
<dependency>
    <groupId>javax.servlet</groupId>
    <artifactId>javax.servlet-api</artifactId>
    <version>4.0.1</version>
</dependency>

<!-- Jersey para compatibilidad -->
<dependency>
    <groupId>org.glassfish.jersey.core</groupId>
    <artifactId>jersey-server</artifactId>
    <version>2.40</version>
</dependency>
```

### ğŸ”§ ConfiguraciÃ³n Obligatoria para Java 17

#### JVM Arguments Requeridos

El mÃ³dulo system de Java 17 requiere flags especiales para permitir acceso a mÃ³dulos internos:

**En pom.xml (para `mvn spring-boot:run`):**
```xml
<plugin>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-maven-plugin</artifactId>
    <configuration>
        <jvmArguments>
            --add-opens java.base/java.lang=ALL-UNNAMED
            --add-opens java.base/sun.nio.ch=ALL-UNNAMED
            --add-opens java.base/sun.util.calendar=ALL-UNNAMED
        </jvmArguments>
    </configuration>
</plugin>
```

**Para ejecutar JAR directamente:**
```bash
java --add-opens java.base/java.lang=ALL-UNNAMED \
     --add-opens java.base/sun.nio.ch=ALL-UNNAMED \
     --add-opens java.base/sun.util.calendar=ALL-UNNAMED \
     -jar target/analytics-1.0.0.jar --spring.profiles.active=local
```

**Error sin estos argumentos:**
```
IllegalAccessError: class org.apache.spark.storage.StorageUtils$ cannot access
class sun.nio.ch.DirectBuffer because module java.base does not export sun.nio.ch
```

#### Spark UI Deshabilitada

- **RazÃ³n**: Conflictos irresolubles entre servlets de Spark UI y Spring Boot 3
- **ConfiguraciÃ³n**: `spark.ui.enabled=false` en SparkConfig.java
- **Impacto**: No habrÃ¡ acceso a Spark UI web (http://localhost:4040)

```java
@Bean
@Profile("local")
public SparkSession sparkSessionLocal() {
    return SparkSession.builder()
            .config("spark.ui.enabled", "false")  // CRÃTICO: Evita conflictos
            .getOrCreate();
}
```

### ğŸš€ Comandos de EjecuciÃ³n Validados

#### OpciÃ³n 1: Maven (Desarrollo)
```bash
# Los JVM arguments se aplican automÃ¡ticamente desde pom.xml
mvn clean spring-boot:run -Dspring-boot.run.profiles=local
```

#### OpciÃ³n 2: JAR (ProducciÃ³n)
```bash
# 1. Compilar
mvn clean package -DskipTests

# 2. Ejecutar con JVM arguments
export JAVA_HOME=~/.sdkman/candidates/java/17.0.13-tem
export PATH=$JAVA_HOME/bin:$PATH

java --add-opens java.base/java.lang=ALL-UNNAMED \
     --add-opens java.base/sun.nio.ch=ALL-UNNAMED \
     --add-opens java.base/sun.util.calendar=ALL-UNNAMED \
     -jar target/analytics-1.0.0.jar \
     --spring.profiles.active=local
```

#### OpciÃ³n 3: Script Automatizado (Recomendado)
```bash
# Usar el script de inicio incluido
./start.sh
```

### ğŸ› Troubleshooting ComÃºn

#### Problema 1: Error de MÃ³dulos Java
**Error:**
```
IllegalAccessError: cannot access class sun.nio.ch.DirectBuffer
```
**SoluciÃ³n:** Usar los JVM arguments `--add-opens` (ver arriba)

#### Problema 2: ANTLR Version Mismatch
**Error:**
```
Could not deserialize ATN with version 3 (expected 4)
```
**SoluciÃ³n:** Verificar `<dependencyManagement>` en pom.xml fuerza ANTLR 4.9.3

#### Problema 3: NoClassDefFoundError javax.servlet
**Error:**
```
NoClassDefFoundError: javax/servlet/Servlet
```
**SoluciÃ³n:** Agregar dependencias `javax.servlet-api` y Jersey (ver pom.xml)

#### Problema 4: Puerto 8080 Ocupado
**Error:**
```
Port 8080 is already in use
```
**SoluciÃ³n:**
```bash
# OpciÃ³n A: Liberar puerto
lsof -ti:8080 | xargs kill -9

# OpciÃ³n B: Cambiar puerto en application.yml
server:
  port: 8081
```

#### Problema 5: No Encuentra CSVs
**Error:**
```
FileNotFoundException: ./data/transactions.csv
```
**SoluciÃ³n:**
```bash
# Verificar que el directorio existe
ls -la data/

# Verificar ruta en application-local.yml
spark:
  data:
    path: ./data  # Ruta relativa desde donde se ejecuta
```

---

## ğŸš€ PrÃ³ximos Pasos: Bloque 2

En el siguiente bloque aprenderemos:

- âœ¨ **Transformaciones avanzadas**: select, filter, groupBy, orderBy
- ğŸ”— **Joins** entre datasets
- ğŸ“Š **Agregaciones complejas**: ventas por categorÃ­a, top productos
- ğŸ” **Spark SQL**: Escribir queries SQL en Spark
- ğŸ¯ **Window Functions**: CÃ¡lculos por ventanas de datos

---

## ğŸ“š Referencias Adicionales

### DocumentaciÃ³n Oficial
- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Spark Java API](https://spark.apache.org/docs/latest/api/java/index.html)
- [Spring Boot Reference](https://docs.spring.io/spring-boot/docs/current/reference/html/)

### Cheat Sheets
- [Spark DataFrame Operations](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/Dataset.html)
- [Spark SQL Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html)

---

**VersiÃ³n del Documento:** 1.1
**Ãšltima ActualizaciÃ³n:** Octubre 2025 - Agregada secciÃ³n "Consideraciones TÃ©cnicas CrÃ­ticas"
**Autor:** Equipo de CapacitaciÃ³n Spark

**Cambios en v1.1:**
- âœ… Agregada secciÃ³n "Consideraciones TÃ©cnicas CrÃ­ticas"
- âœ… DocumentaciÃ³n de conflictos de dependencias (ANTLR, javax/jakarta)
- âœ… Instrucciones detalladas de ejecuciÃ³n con JVM arguments
- âœ… Troubleshooting de errores comunes validados
- âœ… Comandos de ejecuciÃ³n para Maven, JAR y script automatizado

---

## â“ Â¿Preguntas Frecuentes?

### Â¿Por quÃ© Java 17 y no Java 21?
Spark 3.5.0 tiene problemas de compatibilidad con Java 18+. Spark 4.0 (prÃ³ximo) soportarÃ¡ Java 21.

### Â¿Puedo usar Python en lugar de Java?
SÃ­, Spark tiene API para Python (PySpark), pero este curso se enfoca en Java + Spring Boot.

### Â¿Es necesario Docker?
Para este bloque no es estrictamente necesario. Docker se usarÃ¡ mÃ¡s en bloques avanzados para simular clusters.

### Â¿QuÃ© pasa si no tengo 8GB de RAM?
Spark en modo local puede funcionar con 4GB, pero el rendimiento serÃ¡ mÃ¡s lento.

### Â¿Por quÃ© necesito los JVM arguments?
Java 17 tiene mÃ³dulos restrictivos. Los `--add-opens` permiten a Spark acceder a APIs internas necesarias para su funcionamiento.

### Â¿DÃ³nde estÃ¡ la Spark UI?
EstÃ¡ deshabilitada (`spark.ui.enabled=false`) para evitar conflictos de servlets con Spring Boot 3. En producciÃ³n se usarÃ­a un cluster Spark separado.

---

**Â¡Felicitaciones por completar el Bloque 1! ğŸ‰**

Ahora tienes las bases para trabajar con Apache Spark y Spring Boot. En el prÃ³ximo bloque profundizaremos en transformaciones y anÃ¡lisis de datos mÃ¡s complejos.
